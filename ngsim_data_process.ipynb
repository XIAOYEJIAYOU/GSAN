{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../data/US-101-LosAngeles-CA/us-101-vehicle-trajectory-data/vehicle-trajectory-data/0820am-0835am/trajectories-0820am-0835am.csv\n",
      "(1048575, 6) (1048575,) Counter({0.0: 1047910, 1.0: 665})\n",
      "0.923280226720304\n",
      "\n",
      "\n",
      "../../../data/US-101-LosAngeles-CA/us-101-vehicle-trajectory-data/vehicle-trajectory-data/0805am-0820am/trajectories-0805am-0820am.csv\n",
      "(1048575, 6) (1048575,) Counter({0.0: 1047866, 1.0: 709})\n",
      "0.9531038368602975\n",
      "\n",
      "\n",
      "../../../data/US-101-LosAngeles-CA/us-101-vehicle-trajectory-data/vehicle-trajectory-data/0750am-0805am/trajectories-0750am-0805am.csv\n",
      "(1048575, 6) (1048575,) Counter({0.0: 1047374, 1.0: 1201})\n",
      "0.9529667598633116\n",
      "\n",
      "\n",
      "../../../data/I-80-Emeryville-CA/vehicle-trajectory-data/0500pm-0515pm/trajectories-0500-0515.csv\n",
      "(1048575, 6) (1048575,) Counter({0.0: 1047852, 1.0: 723})\n",
      "0.9742373063106536\n",
      "\n",
      "\n",
      "../../../data/I-80-Emeryville-CA/vehicle-trajectory-data/0400pm-0415pm/trajectories-0400-0415.csv\n",
      "(1048575, 6) (1048575,) Counter({0.0: 1047688, 1.0: 887})\n",
      "0.9922059309888657\n",
      "\n",
      "\n",
      "../../../data/I-80-Emeryville-CA/vehicle-trajectory-data/0515pm-0530pm/trajectories-0515-0530.csv\n",
      "(1048575, 6) (1048575,) Counter({0.0: 1047887, 1.0: 688})\n",
      "0.9876357375248236\n",
      "\n",
      "\n",
      "0.9638774874358245\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "from datatool import ade,vid_is_unique,foot2meter,vehicle2track,reset_idx,getNeighborGraph,graph2seq,get_displacement,train_test_val_split,matlab2dataframe\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "\n",
    "def add_change_label(data):\n",
    "    data = data.sort_values(by=[\"Vehicle_ID\",\"Frame_ID\"])\n",
    "    changes = []\n",
    "    for vid, df in data.groupby(\"Vehicle_ID\"):\n",
    "        lane_id = df.Lane_ID.to_numpy()\n",
    "        diff = lane_id[1:] - lane_id[:-1]\n",
    "        changed_or_not = (diff!=0)*1 # 1 : change; 0 : unchange\n",
    "        changed_or_not = np.hstack((np.zeros(1),changed_or_not))\n",
    "        changes.append(changed_or_not)\n",
    "    change_result = np.hstack(changes)\n",
    "    return change_result\n",
    "\n",
    "def graph2seq(data,graph_list,seq_length=16,max_vnum=30,down_sample_rate=5,sort_func=\"distance\"):\n",
    "    x,y,v_id,f_id,l = data.Local_X,data.Local_Y,data.Vehicle_ID,data.Frame_ID,data.label\n",
    "    vehicle_num, frame_num = v_id.max()+1, f_id.max()+1\n",
    "    sparse_X = csr_matrix((x, (v_id, f_id)), shape=(int(vehicle_num), int(frame_num))) # i行:车id;j列:时间;元素为i车j时刻的坐标x\n",
    "    sparse_Y = csr_matrix((y, (v_id, f_id)), shape=(int(vehicle_num), int(frame_num))) # i行:车id;j列:时间;元素为i车j时刻的坐标y\n",
    "    sparse_L = csr_matrix((l, (v_id, f_id)), shape=(int(vehicle_num), int(frame_num))) # i行:车id;j列:时间;元素为i车j时刻的下一时刻是否lane change\n",
    "    seq_windows,label = [],[]\n",
    "    for v,graph in enumerate(graph_list):\n",
    "        if graph.data.size==0:\n",
    "            continue\n",
    "        row = np.unique(graph.tocoo().row)\n",
    "        col = np.unique(graph.tocoo().col)\n",
    "        row_start,row_end = row.min(), row.max()+1\n",
    "        col_start,col_end = col.min(), col.max()+1\n",
    "        dense_v = v - row_start\n",
    "        dense_I = graph[row_start:row_end,col_start:col_end].toarray()\n",
    "        dense_x = sparse_X[row_start:row_end,col_start:col_end].toarray()\n",
    "        dense_y = sparse_Y[row_start:row_end,col_start:col_end].toarray()\n",
    "        dense_xy = np.stack((dense_x,dense_y),axis=2) # (vum,total_seq,2)\n",
    "        dense_l = sparse_L[row_start:row_end,col_start:col_end].toarray()\n",
    "        if dense_xy.shape[0]<max_vnum:\n",
    "            padding_num = max_vnum-dense_xy.shape[0]\n",
    "            padding_xy = np.zeros((padding_num,dense_xy.shape[1],dense_xy.shape[2]))\n",
    "            padding_I = np.zeros((padding_num,dense_I.shape[1]))\n",
    "            dense_xy = np.vstack([dense_xy,padding_xy])\n",
    "            dense_I = np.vstack([dense_I,padding_I])\n",
    "            dense_l = np.vstack([dense_l,padding_I])\n",
    "        for i in range(dense_xy.shape[1]): # for loop on sequence dim\n",
    "            if (i+seq_length)*down_sample_rate > dense_xy.shape[1]:\n",
    "                break\n",
    "            window = dense_xy[:,i:i+seq_length*down_sample_rate:down_sample_rate,:] # (vum=30,seq=16,2)\n",
    "            window_l = dense_l[:,i:i+seq_length*down_sample_rate:down_sample_rate] # (vum=30,seq=16)\n",
    "            if sort_func == \"duration\":\n",
    "                dense_seq_I = dense_I[:,i:(i+seq_length)*down_sample_rate:down_sample_rate]\n",
    "                related_score = dense_seq_I.sum(axis=1)\n",
    "                related_score[dense_v] = related_score[dense_v] + 100 # actually 1 is enough\n",
    "                related_rank = np.argsort(-related_score)\n",
    "            elif sort_func == \"distance\":\n",
    "                related_score = ade(window[:,:6,:],window[dense_v,:6,:])\n",
    "                related_rank = np.argsort(related_score)\n",
    "            window = window[related_rank[:max_vnum],:,:]    \n",
    "            seq_windows.append(window)\n",
    "            label.append(window_l[0,6])\n",
    "    if len(seq_windows)==0:\n",
    "        seq_data = None\n",
    "        seq_label = None\n",
    "    else:\n",
    "        seq_data = np.stack(seq_windows)#(n,vum=30,seq=16,2)\n",
    "        seq_label = np.stack(label)\n",
    "    return seq_data,seq_label\n",
    "# 1\n",
    "root_folder = \"../../../data/\"\n",
    "# 2\n",
    "us101_folder = \"US-101-LosAngeles-CA/us-101-vehicle-trajectory-data/vehicle-trajectory-data\"\n",
    "i80_folder = \"I-80-Emeryville-CA/vehicle-trajectory-data\"\n",
    "#3\n",
    "us101_f1 = \"0820am-0835am/trajectories-0820am-0835am.csv\"\n",
    "us101_f2 = \"0805am-0820am/trajectories-0805am-0820am.csv\"\n",
    "us101_f3 = \"0750am-0805am/trajectories-0750am-0805am.csv\"\n",
    "\n",
    "i80_f1 = \"0500pm-0515pm/trajectories-0500-0515.csv\"\n",
    "i80_f2 = \"0400pm-0415pm/trajectories-0400-0415.csv\"\n",
    "i80_f3 = \"0515pm-0530pm/trajectories-0515-0530.csv\"\n",
    "\n",
    "path_list = []\n",
    "path_list.append(os.path.join(root_folder,us101_folder,us101_f1))\n",
    "path_list.append(os.path.join(root_folder,us101_folder,us101_f2))\n",
    "path_list.append(os.path.join(root_folder,us101_folder,us101_f3))\n",
    "path_list.append(os.path.join(root_folder,i80_folder,i80_f1))\n",
    "path_list.append(os.path.join(root_folder,i80_folder,i80_f2))\n",
    "path_list.append(os.path.join(root_folder,i80_folder,i80_f3))\n",
    "\n",
    "\n",
    "seq_data_list, seq_label_list = [], []\n",
    "for path in path_list:\n",
    "    print(path)\n",
    "    data = pd.read_csv(path)\n",
    "    useful_data = data[[\"Vehicle_ID\",\"Frame_ID\",\"Local_X\",\"Local_Y\",\"Lane_ID\"]]\n",
    "    useful_data = useful_data.sort_values(by=[\"Vehicle_ID\",\"Frame_ID\"])\n",
    "    label = add_change_label(useful_data)\n",
    "    useful_data[\"label\"] = pd.Series(label)\n",
    "    print(useful_data.shape,label.shape,Counter(label))\n",
    "    begin = useful_data[useful_data[\"label\"]==1].index - 20\n",
    "    end = useful_data[useful_data[\"label\"]==1].index + 20\n",
    "    new_label = np.zeros(useful_data.shape[0])\n",
    "    for b,e in zip(begin,end):\n",
    "        new_label[b:e] = 1\n",
    "    useful_data['label'] = new_label\n",
    "    useful_data = reset_idx(useful_data)\n",
    "    useful_data = foot2meter(useful_data)\n",
    "    neighbor_graph = getNeighborGraph(useful_data,radius=50) \n",
    "    seq_data,seq_label = graph2seq(useful_data,neighbor_graph,seq_length=17)\n",
    "    seq_data_list.append(seq_data)\n",
    "    seq_label_list.append(seq_label)\n",
    "    print(Counter(seq_label)[0]/(Counter(seq_label)[0]+Counter(seq_label)[1]))\n",
    "    print(\"\\n\")\n",
    "\n",
    "seq_data_array = np.concatenate(seq_data_list)\n",
    "seq_label_array = np.concatenate(seq_label_list)\n",
    "print(Counter(seq_label_array)[0]/(Counter(seq_label_array)[0]+Counter(seq_label_array)[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1107315, 30, 17, 2), (1107315,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_data_array.shape, seq_label_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39999, 30, 17, 2) (1067316, 30, 17, 2)\n"
     ]
    }
   ],
   "source": [
    "positive_samples = seq_data_array[seq_label_array==1]\n",
    "negative_samples = seq_data_array[seq_label_array==0]\n",
    "print(positive_samples.shape,negative_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_sample_number = positive_samples.shape[0]\n",
    "negative_sample_number = negative_samples.shape[0]\n",
    "for i in range(negative_sample_number//positive_sample_number):\n",
    "    neg_data_seg = negative_samples[positive_sample_number*i:positive_sample_number*(i+1)]\n",
    "    ensemble_data_seg = np.vstack((neg_data_seg,positive_samples))\n",
    "    label = np.hstack((np.zeros(neg_data_seg.shape[0]),np.ones(positive_samples.shape[0])))\n",
    "    save = {\n",
    "        \"data\":ensemble_data_seg,\n",
    "        \"label\":label,\n",
    "    }\n",
    "    with open(f\"data/data_{i}.pkl\",\"wb\") as f:\n",
    "        pkl.dump(save,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"data\"\n",
    "file_list = [f for f in os.listdir(folder) if not f.startswith('.')]\n",
    "for file in file_list:\n",
    "    path = os.path.join(folder,file)\n",
    "    with open(path,\"rb\") as f:\n",
    "        data_dict = pkl.load(f)\n",
    "    data = data_dict['data']\n",
    "    label = data_dict['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}